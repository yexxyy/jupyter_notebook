{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "After 1 training steps,loss=3.760038\n",
      "After 1001 training steps,loss=0.468802\n",
      "After 2001 training steps,loss=0.426766\n",
      "After 3001 training steps,loss=0.207890\n",
      "After 4001 training steps,loss=0.140158\n",
      "After 5001 training steps,loss=0.123904\n",
      "After 6001 training steps,loss=0.156228\n",
      "After 7001 training steps,loss=0.122877\n",
      "After 8001 training steps,loss=0.102882\n",
      "After 9001 training steps,loss=0.118275\n",
      "After 10001 training steps,loss=0.111574\n",
      "After 11001 training steps,loss=0.106098\n",
      "After 12001 training steps,loss=0.114823\n",
      "After 13001 training steps,loss=0.091506\n",
      "After 14001 training steps,loss=0.128570\n",
      "After 15001 training steps,loss=0.090939\n",
      "After 16001 training steps,loss=0.140453\n",
      "After 17001 training steps,loss=0.109333\n",
      "After 18001 training steps,loss=0.095992\n",
      "After 19001 training steps,loss=0.115785\n",
      "After 20001 training steps,loss=0.085241\n",
      "After 21001 training steps,loss=0.105308\n",
      "After 22001 training steps,loss=0.096214\n",
      "After 23001 training steps,loss=0.091049\n",
      "After 24001 training steps,loss=0.113459\n",
      "After 25001 training steps,loss=0.099732\n",
      "After 26001 training steps,loss=0.089094\n",
      "After 27001 training steps,loss=0.092756\n",
      "After 28001 training steps,loss=0.108836\n",
      "After 29001 training steps,loss=0.108840\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import ipynb_importer\n",
    "import tensorflow_5_5_mnist_inference as mnist_inference\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "BATCH_SIZE=100\n",
    "LEARNING_RATE_BASE=0.8\n",
    "LEARNING_RATE_DECAY=0.99\n",
    "REGULARAZTION_RATE=0.0001\n",
    "TRAINING_STEPS=30000\n",
    "MOVING_AVERAGE_DECAY=0.99\n",
    "\n",
    "MODEL_SAVE_PATH='./tmp/best_example/'\n",
    "MODEL_NAME='model.ckpt'\n",
    "\n",
    "def train(mnist):\n",
    "    x=tf.placeholder(tf.float32,[None,mnist_inference.INPUT_NODE],name='x-input')\n",
    "    y_=tf.placeholder(tf.float32,[None,mnist_inference.OUTPUT_NODE],name='y-input')\n",
    "    \n",
    "    regularizer=tf.contrib.layers.l2_regularizer(REGULARAZTION_RATE)\n",
    "    \n",
    "    #向前传播\n",
    "    y=mnist_inference.inference(x,regularizer)\n",
    "    global_step=tf.Variable(0,trainable=False)\n",
    "    \n",
    "    variable_averages=tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY,global_step)\n",
    "    variable_averages_op=variable_averages.apply(tf.trainable_variables())\n",
    "    \n",
    "    cross_entropy=tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y,labels=tf.argmax(y_,1))\n",
    "    cross_entropy_mean=tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "    loss=cross_entropy_mean+tf.add_n(tf.get_collection('losses'))\n",
    "    learning_rate=tf.train.exponential_decay(\n",
    "        LEARNING_RATE_BASE,\n",
    "        global_step,\n",
    "        mnist.train.num_examples/BATCH_SIZE,\n",
    "        LEARNING_RATE_DECAY\n",
    "    )\n",
    "    train_step=tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step=global_step)\n",
    "    \n",
    "    with tf.control_dependencies([train_step,variable_averages_op]):\n",
    "        train_op=tf.no_op(name='train')\n",
    "        \n",
    "    saver=tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for i in range(TRAINING_STEPS):\n",
    "            xs,ys=mnist.train.next_batch(BATCH_SIZE)\n",
    "            _,loss_value,step=sess.run([train_op,loss,global_step],feed_dict={x:xs,y_:ys})\n",
    "            \n",
    "            #每1000轮保存一次模型\n",
    "            if i % 1000 ==0:\n",
    "                print 'After %d training steps,loss=%f' % (step,loss_value)\n",
    "                \n",
    "                #保存模型\n",
    "                saver.save(sess,os.path.join(MODEL_SAVE_PATH,MODEL_NAME),global_step=step)\n",
    "                \n",
    "                \n",
    "def main(argv=None):\n",
    "    mnist=input_data.read_data_sets('./MNIST_data',one_hot=True)\n",
    "    train(mnist)\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    \"\"\"\n",
    "    ValueError: Variable layer1/weights already exists, disallowed. \n",
    "    Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n",
    "    \"\"\"\n",
    "    tf.get_variable_scope().reuse_variables()\n",
    "    \n",
    "    tf.app.run()\n",
    "            \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
