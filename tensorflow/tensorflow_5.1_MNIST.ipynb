{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "After 0 training steps,validation accuracy 0.105000\n",
      "After 1000 training steps,validation accuracy 0.976000\n",
      "After 2000 training steps,validation accuracy 0.981000\n",
      "After 3000 training steps,validation accuracy 0.981400\n",
      "After 4000 training steps,validation accuracy 0.981200\n",
      "After 5000 training steps,validation accuracy 0.982800\n",
      "After 6000 training steps,validation accuracy 0.983600\n",
      "After 7000 training steps,validation accuracy 0.983400\n",
      "After 8000 training steps,validation accuracy 0.984600\n",
      "After 9000 training steps,validation accuracy 0.984200\n",
      "After 10000 training steps,validation accuracy 0.984000\n",
      "After 11000 training steps,validation accuracy 0.985000\n",
      "After 12000 training steps,validation accuracy 0.984400\n",
      "After 13000 training steps,validation accuracy 0.984000\n",
      "After 14000 training steps,validation accuracy 0.984000\n",
      "After 15000 training steps,validation accuracy 0.983600\n",
      "After 16000 training steps,validation accuracy 0.983800\n",
      "After 17000 training steps,validation accuracy 0.983800\n",
      "After 18000 training steps,validation accuracy 0.983600\n",
      "After 19000 training steps,validation accuracy 0.984000\n",
      "After 20000 training steps,validation accuracy 0.983800\n",
      "After 21000 training steps,validation accuracy 0.984000\n",
      "After 22000 training steps,validation accuracy 0.983800\n",
      "After 23000 training steps,validation accuracy 0.983200\n",
      "After 24000 training steps,validation accuracy 0.983600\n",
      "After 25000 training steps,validation accuracy 0.984000\n",
      "After 26000 training steps,validation accuracy 0.983000\n",
      "After 27000 training steps,validation accuracy 0.984000\n",
      "After 28000 training steps,validation accuracy 0.983800\n",
      "After 29000 training steps,validation accuracy 0.983600\n",
      "Test accuracy:0.983200\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "INPUT_NODE=784\n",
    "OUTPUT_NODE=10\n",
    "LAYER1_NODE=500\n",
    "BATCH_SIZE=100\n",
    "\n",
    "#学习率与学习衰减率\n",
    "LEARNING_RATE_BASE=0.8\n",
    "LEARNING_RATE_DACAY=0.99\n",
    "\n",
    "REGULARIZATION_RATE=0.0001 #描述模型复杂度的正则化项在损失函数中的系数\n",
    "TRAINING_STEPS=30000\n",
    "MOVING_AVERAGE_DECAY=0.99  #滑动平均衰减率\n",
    "\n",
    "def inference(input_tensor,avg_class,weights1,biases1,weights2,biases2):\n",
    "    \n",
    "    #没有提供滑动平均类，直接使用参数当前取值\n",
    "    if avg_class is None:\n",
    "        layer1=tf.nn.relu(tf.matmul(input_tensor,weights1)+biases1)\n",
    "        return tf.matmul(layer1,weights2)+biases2\n",
    "    else:\n",
    "        layer1=tf.nn.relu(\n",
    "            tf.matmul(input_tensor,avg_class.average(weights1)) + avg_class.average(biases1)\n",
    "        )\n",
    "        return tf.matmul(layer1,avg_class.average(weights2)) + avg_class.average(biases2)\n",
    "    \n",
    "    \n",
    "def train(mnist):\n",
    "    x=tf.placeholder(tf.float32,[None,INPUT_NODE],name='x-input')\n",
    "    y_=tf.placeholder(tf.float32,[None,OUTPUT_NODE],name='y-input')\n",
    "    \n",
    "    weights1=tf.Variable(tf.truncated_normal([INPUT_NODE,LAYER1_NODE],stddev=0.1))\n",
    "    biases1=tf.Variable(tf.constant(0.1,shape=[LAYER1_NODE]))\n",
    "    \n",
    "    weights2=tf.Variable(tf.truncated_normal([LAYER1_NODE,OUTPUT_NODE],stddev=0.1))\n",
    "    biases2=tf.Variable(tf.constant(0.1,shape=[OUTPUT_NODE]))\n",
    "    \n",
    "    y=inference(x,None,weights1,biases1,weights2,biases2)\n",
    "    \n",
    "    global_step=tf.Variable(0,trainable=False)\n",
    "    variable_averages=tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY,global_step)\n",
    "    variable_averages_op=variable_averages.apply(tf.trainable_variables())\n",
    "    \n",
    "    \n",
    "    average_y=inference(x,variable_averages,weights1,biases1,weights2,biases2)\n",
    "    \n",
    "    cross_entropy=tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y,labels=tf.arg_max(y_,1))\n",
    "    cross_entropy_mean=tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "    regularizer=tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "    regularation=regularizer(weights1)+ regularizer(weights2)\n",
    "    \n",
    "    loss=cross_entropy_mean + regularation\n",
    "    \n",
    "    learning_rate=tf.train.exponential_decay(\n",
    "        LEARNING_RATE_BASE,\n",
    "        global_step,\n",
    "        mnist.train.num_examples/BATCH_SIZE,\n",
    "        LEARNING_RATE_DACAY\n",
    "    )\n",
    "    \n",
    "    train_step=tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step=global_step)\n",
    "    \n",
    "    with tf.control_dependencies([train_step,variable_averages_op]):\n",
    "        train_op=tf.no_op(name='train')\n",
    "    \n",
    "    correct_prediction=tf.equal(tf.arg_max(average_y,1),tf.arg_max(y_,1))\n",
    "    accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "    \n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        validate_feed={x:mnist.validation.images,y_:mnist.validation.labels}\n",
    "\n",
    "        test_feed={x:mnist.test.images,y_:mnist.test.labels}\n",
    "\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            if i%1000==0:\n",
    "                validate_acc=sess.run(accuracy,feed_dict=validate_feed)\n",
    "                print 'After %d training steps,validation accuracy %f' % (i,validate_acc)\n",
    "\n",
    "            xs,ys=mnist.train.next_batch(BATCH_SIZE)\n",
    "            sess.run(train_op,feed_dict={x:xs,y_:ys})\n",
    "\n",
    "        test_acc=sess.run(accuracy,feed_dict=test_feed)\n",
    "        print 'Test accuracy: %f' % test_acc\n",
    "        \n",
    "\n",
    "def main(argv=None):\n",
    "    mnist=input_data.read_data_sets('/tmp/data',one_hot=True)\n",
    "    train(mnist)\n",
    "    \n",
    "    \n",
    "if __name__=='__main__':\n",
    "    tf.app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
